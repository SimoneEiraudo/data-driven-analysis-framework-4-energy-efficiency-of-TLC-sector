{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to support supervision of the training procuedure of regression models (previously performed by means of scripts blabkbox_model_training.py, ES_model_training.py and semiPar_model_training.py) by providing visual elaboration of the performance metrics, predictions and residual errors of such models. Besides, the internal architecture of models is visualized. \n",
    "\n",
    "This notebook provides quick insights into the training procedure, to make ensure everything is going smoothly, while a more in depth analysis of regression models predictions and performance, and a comparative of the different approaches, is available in notebook models_comparison.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import itertools \n",
    "import seaborn as sns\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "PROJECT_DIR =Path(os.path.abspath('')).parents[1]\n",
    "sys.path.append(os.fspath(PROJECT_DIR))\n",
    "#sys.path.insert(0,'../')  # add previous directory to path to load constants module\n",
    "\n",
    "#from MLP_fx import create_model, \n",
    "from pipeline.regression_fx import realElbows\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "\n",
    "from pipeline.definitions import *\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select graphic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_setting=\"notebook\" #or \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph_setting==\"article\":\n",
    "    \n",
    "    #journal-quality parameter settings\n",
    "    resolution_factor=2\n",
    "    desired_font=10\n",
    "\n",
    "elif graph_setting==\"notebook\":\n",
    "    resolution_factor=1\n",
    "    desired_font=12\n",
    "    \n",
    "#conversion factors\n",
    "cm_to_inch=0.393701\n",
    "classic_proportion=6.4/4.8\n",
    "golden_rate=1.618\n",
    "\n",
    "#Elsevier column width is 8.4 cm, double-column width is 17.7 cm (in inches: 3.31 and 6.97)\n",
    "small_figsize=(resolution_factor*3.31, resolution_factor*3.31/classic_proportion)\n",
    "big_figsize=(resolution_factor*6.97, resolution_factor*6.97/classic_proportion)\n",
    "\n",
    "#changings regarding fonttypex\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = \"Arial\"\n",
    "\n",
    "font_size=resolution_factor*desired_font\n",
    "\n",
    "\n",
    "#define path for figures\n",
    "figures_path=FIGURES\n",
    "#check existance of figure path\n",
    "if not os.path.exists(figures_path):\n",
    "    print(\"The selected directory to store figures does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, models and scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a case study dataset using pandas\n",
    "method_short=\"ts\" #that is, time serie, or ew, that is elementWise\n",
    "dataset = pd.read_csv(os.path.join(DATA_NORMALIZED, 'norm_'+method_short+'_buildings_dataset.csv'))\n",
    "print(\"Shape of dataset: \"+str(dataset.shape))\n",
    "#load dataset description\n",
    "data_info=pd.read_csv(DATASETS+'/buildings_data_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs=dataset.ID.unique()\n",
    "models_name_list=[\"Parametric\", \"Semi-Parametric\", \"Non-Parametric\"]\n",
    "approaches=[\"uniES\", \"semiPar\",  \"blackbox\"]\n",
    "models_list=['P_phys', 'P_semiPar', 'P_blackbox']\n",
    "##load scalers\n",
    "file_name=\"scalers_\"+method_short\n",
    "with open(DATA+\"/\"+file_name+\".pkl\", 'rb') as f:\n",
    "    scalers_dict = pickle.load(f)\n",
    "#create a models dictionary (keys: approach, casestudy, model_n)\n",
    "models={}\n",
    "#create a dictionary for the predictions (keys: approach, casestudy, model_n)\n",
    "predictions={}\n",
    "NN_predictions={}\n",
    "#create dataframes to store performance metrics from the models\n",
    "df_r2=pd.DataFrame(index=pd.MultiIndex.from_product([models_name_list, ['Train', 'Test']]), columns=pd.MultiIndex.from_product([IDs.tolist(), [ \"model_\"+str(x) for x in range(10) ]]))\n",
    "df_mape=df_r2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare input matrices for the different approaches\n",
    "ind_var=[\"T_a\", \"RH\", \"Wv\", \"atmP\", \"G\", \"s_Wa\", \"c_Wa\", \"s_H\", \"c_H\", \"s_D\", \"c_D\", \"dayType\"]\n",
    "n_inputs=len(ind_var)\n",
    "modeled_ind_var=\"T_a\"\n",
    "dep_var=\"P\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for approach, models_path in zip(approaches, [PAR_MODELS, SEMIPAR_MODELS, BLACKBOX_MODELS]):\n",
    "    approach_models=[f for f in listdir(models_path) if isfile(join(models_path, f))]\n",
    "    for caseStudy in IDs:\n",
    "        caseStudy_models_list=[f for f in approach_models if caseStudy in f]\n",
    "        for model, model_n in zip(caseStudy_models_list, [ \"model_\"+str(x) for x in range(len(caseStudy_models_list)) ]):\n",
    "            if (approach==\"semiPar\") | (approach==\"uniES\"):\n",
    "                models[approach, caseStudy, model_n]=tf.keras.models.load_model(models_path+'/'+model, custom_objects={'realElbows':realElbows}, compile=False)\n",
    "            else:\n",
    "                models[approach, caseStudy, model_n]=tf.keras.models.load_model(models_path+'/'+model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the architeture of the semiPar model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudy='building_1001'\n",
    "keras.utils.plot_model(models[\"semiPar\", caseStudy, model_n], show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach, approach_name, n_required in zip(approaches, models_name_list, [len(IDs), len(IDs), len(IDs)]):\n",
    "    approach_models_names=[model for model in models if approach in model]\n",
    "    n_models=len(approach_models_names)\n",
    "    if n_models==n_required:\n",
    "        print(colored(\"SUCCESSFUL MODEL LOADING: \"+str(n_required)+\" \"+approach_name+\" models have been loaded\", \"green\"))\n",
    "    elif n_models<n_required:\n",
    "        print(colored(\"ERROR - MODELS MISSING: \"+str(n_models)+\" \"+approach_name+\" models have been loaded, but \"+str(n_required)+\" models were expected\", \"red\"))\n",
    "    else:\n",
    "        print(colored(\"ERROR - TOO MANY MODELS: \"+str(n_models)+\" \"+approach_name+\" models have been loaded, but \"+str(n_required)+\" models were expected\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models performance assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce and store predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach, approach_name in zip(approaches, models_name_list):\n",
    "    #approach_models=[f for f in allmodels if approach in f]\n",
    "    for caseStudy in IDs:\n",
    "        #caseStudy_models_list=[f for f in approach_models if caseStudy in f]\n",
    "        \n",
    "        #define modeled input\n",
    "        df=dataset.loc[dataset.ID==caseStudy]\n",
    "        x=pd.DataFrame(df[ind_var])\n",
    "        x_modeled=pd.DataFrame(df[modeled_ind_var])\n",
    "        y=pd.DataFrame(df[dep_var])\n",
    "        \n",
    "        #split test and train \n",
    "        if method_short== \"ts\":\n",
    "            x_train, x_test, y_train,y_test= train_test_split(x, y, test_size=0.33, shuffle=False)\n",
    "        elif method_short== \"ew\":\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "        x_modeled_train=pd.DataFrame(x_train[modeled_ind_var])\n",
    "        x_modeled_test=pd.DataFrame(x_test[modeled_ind_var])\n",
    "\n",
    "        #check number of models to iterate for\n",
    "        approach_models=[model for model in models if approach in model]\n",
    "        caseStudy_models_list=[f for f in approach_models if caseStudy in f]\n",
    "        n_models=len(caseStudy_models_list)\n",
    "        \n",
    "        for n, model_n in zip(range(n_models), [ \"model_\"+str(x) for x in range(n_models) ]):\n",
    "            model=models[approach, caseStudy, model_n]\n",
    "            if approach==\"uniES\":\n",
    "                y_NN_pred=model.predict(x_modeled, verbose=0)\n",
    "\n",
    "            elif approach==\"semiPar\":\n",
    "                y_NN_pred=model.predict((x_modeled, x), verbose=0)\n",
    "\n",
    "            elif approach==\"blackbox\":\n",
    "                y_NN_pred=model.predict(x, verbose=0)\n",
    "\n",
    "            #split test and train \n",
    "            if method_short== \"ts\":\n",
    "                 _,_, y_NN_pred_train, y_NN_pred_test= train_test_split(x, y_NN_pred, test_size=0.33, shuffle=False)\n",
    "            elif method_short== \"ew\":\n",
    "                 _,_, y_NN_pred_train, y_NN_pred_test = train_test_split(x, y_NN_pred, test_size=0.33, random_state=42)\n",
    "        \n",
    "            # rescale real outputs\n",
    "            y_real=scalers_dict[caseStudy, \"y\"].inverse_transform(y)\n",
    "            y_real_train=scalers_dict[caseStudy, \"y\"].inverse_transform(y_train)\n",
    "            y_real_test=scalers_dict[caseStudy, \"y\"].inverse_transform(y_test)\n",
    "            \n",
    "            #predictions of the full set\n",
    "            y_pred=scalers_dict[caseStudy, \"y\"].inverse_transform(y_NN_pred)\n",
    "            predictions[approach, caseStudy, model_n]=y_pred\n",
    "\n",
    "            #store non-rescaled predictions\n",
    "            NN_predictions[approach, caseStudy, model_n, \"Train\"]=y_NN_pred_train\n",
    "            NN_predictions[approach, caseStudy, model_n, \"Test\"]=y_NN_pred_test\n",
    "            NN_predictions[approach, caseStudy, model_n, \"Whole Set\"]=y_NN_pred\n",
    "            \n",
    "            \n",
    "            #train set predictions and scores\n",
    "            y_pred_train=scalers_dict[caseStudy, \"y\"].inverse_transform(y_NN_pred_train)\n",
    "            df_r2.loc[approach_name, 'Train'][caseStudy, model_n] =r2_score(y_real_train, y_pred_train)\n",
    "            df_mape.loc[approach_name, 'Train'][caseStudy, model_n]=mean_absolute_percentage_error(y_real_train, y_pred_train)\n",
    "            \n",
    "            #test set predictions and scores\n",
    "            y_pred_test=scalers_dict[caseStudy, \"y\"].inverse_transform(y_NN_pred_test)\n",
    "            df_r2.loc[approach_name, 'Test'][caseStudy, model_n] =r2_score(y_real_test, y_pred_test)\n",
    "            df_mape.loc[approach_name, 'Test'][caseStudy, model_n]=mean_absolute_percentage_error(y_real_test, y_pred_test)\n",
    "            \n",
    "    print(colored(\"Predictions from the \"+approach_name+\" models correctly retrieved\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance indexes overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize graphical object\n",
    "fig, (ax1, ax2) = plt.subplots(2, 3, figsize=big_figsize)\n",
    "subplot_pos_list=[231, 232, 233, 234, 235, 236]\n",
    "\n",
    "for case, subplot_pos in zip(list(itertools.product([df_r2, df_mape], models_name_list)), subplot_pos_list):\n",
    "    df=case[0]\n",
    "    df_test=pd.DataFrame(df.loc[(case[1], \"Test\"), :]).unstack(level=1).T.astype(float)\n",
    "    #access to specific subplot\n",
    "    ax=plt.subplot(subplot_pos)\n",
    "\n",
    "    sns.boxplot(data=df_test)#, y='variable', x='value', hue='group', palette=colors)\n",
    "    \n",
    "    if subplot_pos in [231, 232, 233]:  \n",
    "        ax.set_title(case[1], fontsize=font_size)\n",
    "        ax.set_ylim([0, 1])\n",
    "    else: \n",
    "        ax.set_ylim([0, 0.035])\n",
    "    if subplot_pos==231:\n",
    "        ax.set_ylabel(\"$R^2$\", fontsize=font_size-2, rotation=0)\n",
    "        #ax.yaxis.set_label_coords(-0.25, +0.5)\n",
    "    elif subplot_pos==234:\n",
    "        ax.set_ylabel(\"MAPE\", fontsize=font_size-2, rotation=0)\n",
    "        #ax.yaxis.set_label_coords(-0.25, +0.5)\n",
    "    ax.set_xticklabels([])\n",
    "    if subplot_pos in [232, 233, 235, 236]:\n",
    "       ax.set_yticklabels([]) \n",
    "    ax.grid()\n",
    "    ax.tick_params(labelsize=font_size)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Predictions comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots=2\n",
    "for  caseStudy in IDs[:n_plots]:\n",
    "    #define modeled input\n",
    "    df=dataset.loc[dataset.ID==caseStudy]\n",
    "    y_real=df[dep_var]\n",
    "    \n",
    "    if method_short== \"ts\":\n",
    "        _, _, y_real_train, y_real_test = train_test_split(x, y_real, test_size=0.33, shuffle=False)\n",
    "    elif method_short== \"ew\":\n",
    "        _,_ , y_real_train, y_real_test = train_test_split(x, y_real, test_size=0.33, random_state=42)\n",
    "    \n",
    "    #initialize a new graphical object\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(big_figsize[0], big_figsize[1]/2) )\n",
    "    \n",
    "    for approach, approach_name, ax in zip(approaches, models_name_list, axs):\n",
    "\n",
    "        #check number of models to iterate for\n",
    "        approach_models=[model for model in models if approach in model]\n",
    "        caseStudy_models_list=[f for f in approach_models if caseStudy in f]\n",
    "        n_models=len(caseStudy_models_list)\n",
    "        \n",
    "        n_models=1 #set n_models=1 if you want only the best SemiPar and NonPar model to be displayed, leave it as it was if you want to see all the models\n",
    "        \n",
    "        for n, model_n in zip(range(n_models), [ \"model_\"+str(x) for x in range(n_models) ]):\n",
    "            y_pred_train=NN_predictions[approach, caseStudy, model_n, \"Train\"]\n",
    "            y_pred_test=NN_predictions[approach, caseStudy, model_n, \"Test\"]\n",
    "            #scatter predictions\n",
    "            ax.scatter(y_real_train, y_pred_train, 1.5, alpha=0.2)\n",
    "            ax.scatter(y_real_test, y_pred_test, 1.5, alpha=0.2)\n",
    "            ax.plot([0, 1], [0, 1], color=\"black\", linestyle='--')\n",
    "\n",
    "            #graphical custom options\n",
    "            ax.grid(True, which='both',axis='both',alpha=0.5, linewidth=0.45)\n",
    "            ax.tick_params(labelsize=font_size-2)\n",
    "            ticks=[0, 0.25, 0.5, 0.75, 1]\n",
    "            ax.set_xticks(ticks)\n",
    "            ax.set_yticks(ticks)\n",
    "\n",
    "            if caseStudy!=IDs[n_plots-1]:\n",
    "                ax.tick_params(axis='x',which='both',length=0.1,width=0.1,pad=1)\n",
    "                #ax.set_xticklabels([])\n",
    "\n",
    "            if caseStudy==IDs[0]:\n",
    "                ax.set_title(approach_name, fontsize=font_size)\n",
    "\n",
    "                \n",
    "            #arrange image details (ticks, tickslabels and titles)\n",
    "            if ax!=axs[0]:\n",
    "                    ax.tick_params(axis='y',which='both',length=0.1,width=0.1,pad=1)\n",
    "                    ax.set_yticklabels([])\n",
    "\n",
    "            text_y=fig.supylabel(caseStudy+'\\n\\n'+'Predictions [-]', x=0.02, fontsize=font_size)     \n",
    "            plt.tight_layout()\n",
    "            \n",
    "    if caseStudy==IDs[0]:\n",
    "        axs[2].legend([\"Train\", \"Test\"], fontsize=font_size)\n",
    " \n",
    "text_x=fig.supxlabel('True Values [-]', y=-0.02, fontsize=font_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots=2\n",
    "\n",
    "for  caseStudy in IDs[:n_plots]:\n",
    "    #define modeled input\n",
    "    df=dataset.loc[dataset.ID==caseStudy]\n",
    "    y_real=df[dep_var].values\n",
    " \n",
    "    #initialize a new graphical object\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(big_figsize[0], big_figsize[1]/2))\n",
    "    font_size=12\n",
    "    \n",
    "    for approach, approach_name, ax in zip(approaches, models_name_list, axs):\n",
    "        y_pred=NN_predictions[approach, caseStudy, \"model_0\", \"Whole Set\"].flatten()\n",
    "        res = y_real-y_pred\n",
    "        ax.hist(res, bins=30)\n",
    "\n",
    "        ax.tick_params(labelsize=font_size-2)\n",
    "        #xticks=[-0.5, -0.25 ,0, 0.25, 0.5]\n",
    "        #yticks=[0, 500,  1000, 1500, 2000]\n",
    "        #ax.set_xticks(xticks)\n",
    "        #ax.set_yticks(yticks)\n",
    "\n",
    "        #graphical custom options\n",
    "        if caseStudy!=IDs[n_plots-1]:\n",
    "            ax.tick_params(axis='x',which='both',length=0.1,width=0.1,pad=1)\n",
    "            #ax.set_xticklabels([])\n",
    "\n",
    "        if ax!=axs[0]:\n",
    "                ax.tick_params(axis='y',which='both',length=0.1,width=0.1,pad=1)\n",
    "                #ax.set_yticklabels([])\n",
    "            \n",
    "        if caseStudy==IDs[0]:\n",
    "                ax.set_title(approach_name, fontsize=font_size)\n",
    "            \n",
    "        text_y=fig.supylabel(caseStudy+'\\n\\n'+'Number of occurences', x=-0.02, fontsize=font_size)   \n",
    "\n",
    "    plt.tight_layout()\n",
    "text_x=fig.supxlabel('Error (calculated from normalized values)', y=-0.05, fontsize=font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of correlation of residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select one case study\n",
    "caseStudy=random.choice(IDs)\n",
    "caseStudy=IDs[0]\n",
    "df=dataset.loc[dataset.ID==caseStudy]\n",
    "\n",
    "#add to the matrix the output values (predicted power) and the residuals from the different models\n",
    "df_extended=df.copy()\n",
    "df_extended['P^_ES']=NN_predictions[\"uniES\", caseStudy, \"model_0\", \"Whole Set\"]\n",
    "df_extended['P^_semi']=NN_predictions[\"semiPar\", caseStudy, \"model_0\", \"Whole Set\"]\n",
    "df_extended['P^_bb']=NN_predictions[\"blackbox\", caseStudy, \"model_0\", \"Whole Set\"]\n",
    "df_extended['err_ES']=df_extended[\"P\"]-df_extended[\"P^_ES\"]\n",
    "df_extended['err_semi']=df_extended[\"P\"]-df_extended[\"P^_semi\"]\n",
    "df_extended['err_bb']=df_extended[\"P\"]-df_extended[\"P^_bb\"]\n",
    "\n",
    "#reorder dataframe columns\n",
    "new_order=['P', 'P^_ES', 'P^_semi', 'P^_bb',  'err_ES','err_semi', 'err_bb', 'T_a', 'RH', 'Wv', 'atmP', 'G', 's_Wa', 'c_Wa', 's_H', 'c_H',\n",
    "       's_D', 'c_D', 'dayType', 'ID']\n",
    "df_extended=df_extended[new_order]\n",
    "\n",
    "#calculate correlation\n",
    "df_corr=df_extended.drop(\"ID\", axis=1).corr()\n",
    "\n",
    "#select features to be displayed in the reduced correlation heat map\n",
    "reduced_df_corr=df_corr.loc[['P', 'P^_ES', 'P^_semi', 'P^_bb',  'err_ES','err_semi', 'err_bb'], ['P', 'P^_ES', 'P^_semi', 'P^_bb',  'err_ES','err_semi', 'err_bb', 'T_a', 'RH', 'Wv', 'atmP', 'G', 's_Wa', 'c_Wa', 's_H', 'c_H',\n",
    "       's_D', 'c_D', 'dayType']]\n",
    "\n",
    "#provide one correlation heatmap with labels and colorbar\n",
    "fig, axs=plt.subplots(1, figsize=small_figsize)\n",
    "axs.tick_params(labelsize=font_size-2)\n",
    "sns.heatmap(df_corr, vmin=0.0, vmax=1.0, ax=axs, square=True)\n",
    "plt.title(\"Sample correlation Matrix\", fontsize=font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display all the subcases, without labels and colormpap\n",
    "plots_per_row=2\n",
    "fig, axs=plt.subplots(int(len(IDs)/plots_per_row), plots_per_row, figsize=(4*plots_per_row, 1.2*len(IDs)))\n",
    "for caseStudy, ax, i in zip(IDs, axs.flatten(), range(len(IDs))):\n",
    "    df=dataset.loc[dataset.ID==caseStudy]\n",
    "\n",
    "    #add to the matrix the output values (predicted power) and the residuals from the different models\n",
    "    df_extended=df.copy()\n",
    "    df_extended['P^_ES']=NN_predictions[\"uniES\", caseStudy, \"model_0\", \"Whole Set\"]\n",
    "    df_extended['P^_semi']=NN_predictions[\"semiPar\", caseStudy, \"model_0\", \"Whole Set\"]\n",
    "    df_extended['P^_bb']=NN_predictions[\"blackbox\", caseStudy, \"model_0\", \"Whole Set\"]\n",
    "    df_extended['err_ES']=df_extended[\"P\"]-df_extended[\"P^_ES\"]\n",
    "    df_extended['err_semi']=df_extended[\"P\"]-df_extended[\"P^_semi\"]\n",
    "    df_extended['err_bb']=df_extended[\"P\"]-df_extended[\"P^_bb\"]\n",
    "    \n",
    "    #reorder dataframe columns\n",
    "    new_order=['P', 'P^_ES', 'P^_semi', 'P^_bb',  'err_ES','err_semi', 'err_bb', 'T_a', 'RH', 'Wv', 'atmP', 'G', 's_Wa', 'c_Wa', 's_H', 'c_H',\n",
    "           's_D', 'c_D', 'dayType', 'ID']\n",
    "    df_extended=df_extended[new_order]\n",
    "    \n",
    "    #calculate correlation\n",
    "    df_corr=df_extended.drop(\"ID\", axis=1).corr()\n",
    "    \n",
    "    #select features to be displayed in the reduced correlation heat map\n",
    "    reduced_df_corr=df_corr.loc[['P', 'P^_ES', 'P^_semi', 'P^_bb',  'err_ES','err_semi', 'err_bb'], ['P', 'P^_ES', 'P^_semi', 'P^_bb',  'err_ES','err_semi', 'err_bb', 'T_a', 'RH', 'Wv', 'atmP', 'G', 's_Wa', 'c_Wa', 's_H', 'c_H',\n",
    "       's_D', 'c_D', 'dayType']]\n",
    "    sns.heatmap(reduced_df_corr, vmin=0.0, vmax=1.0, ax=ax, cbar=False)\n",
    "    if (i in plots_per_row*np.linspace(0, len(IDs),1))==False:\n",
    "        ax.set_yticks([])\n",
    "    if i < len(IDs)-plots_per_row:\n",
    "        ax.set_xticks([])\n",
    "\n",
    "    ax.set_title(caseStudy, fontsize=font_size)\n",
    "    \n",
    "plt.suptitle(\"Correlation matrix from all the case studies\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "88688c6e448969839ee4163a6b670935eee153233d49c269b97f67337e0696f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
