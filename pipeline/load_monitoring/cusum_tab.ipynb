{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805432e-ad6c-4b08-a1a7-46acdc136906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "PROJECT_DIR =Path(os.path.abspath('')).parents[1]\n",
    "sys.path.append(os.fspath(PROJECT_DIR))\n",
    "\n",
    "from pipeline.definitions import *\n",
    "from pipeline.preprocessing.data_preprocessing import statistical_prepro\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "import matplotlib.colors as mc\n",
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pipeline.monitoring_fx import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6520d-d046-4e25-95df-e31eea92eca5",
   "metadata": {},
   "source": [
    "## Define general parameters for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f2512-7280-474c-8e9f-eeb98acde156",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_setting=\"notebook\" #or \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef29f6-ffe7-46c2-90fd-c550ea5bcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph_setting==\"article\":\n",
    "    #journal-quality parameter settings\n",
    "    resolution_factor=2\n",
    "    desired_font=10\n",
    "\n",
    "elif graph_setting==\"notebook\":\n",
    "    resolution_factor=1\n",
    "    desired_font=12\n",
    "\n",
    "#conversion factors\n",
    "cm_to_inch=0.393701\n",
    "classic_proportion=6.4/4.8\n",
    "golden_rate=1.618\n",
    "\n",
    "#conversion factors\n",
    "cm_to_inch=0.393701\n",
    "classic_proportion=6.4/4.8\n",
    "golden_rate=1.618\n",
    "\n",
    "#Elsevier column width is 8.4 cm, double-column width is 17.7 cm (in inches: 3.31 and 6.97)\n",
    "small_figsize=(resolution_factor*3.31, resolution_factor*3.31/classic_proportion)\n",
    "big_figsize=(resolution_factor*6.97, resolution_factor*6.97/classic_proportion)\n",
    "#other figure sizes\n",
    "square_figsize=(resolution_factor*3.31, resolution_factor*3.31)\n",
    "long_ts_figsize=(resolution_factor*10.5, resolution_factor*2.4)\n",
    "\n",
    "#define colors palette\n",
    "colors={}\n",
    "colors[\"P_phys\"]=\"sandybrown\" \n",
    "colors[\"P_semiPar\"]=\"indianred\"\n",
    "colors[\"P_blackbox\"]=\"mediumpurple\"\n",
    "\n",
    "#changings regarding fonttypex\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = \"Arial\"\n",
    "\n",
    "font_size=resolution_factor*desired_font\n",
    "\n",
    "#define path for figures\n",
    "figures_path=\"C:/Users/simone.eiraudo/OneDrive - Politecnico di Torino/Comodino/PhD/final defense/images\"\n",
    "#check existance of figure path\n",
    "if not os.path.exists(figures_path):\n",
    "    print(\"The selected directory to store figures does not exist\")\n",
    "\n",
    "#define colors for images\n",
    "colors={}\n",
    "colors[\"P\"]=\"gray\" \n",
    "colors[\"P_pred\"]=\"indianred\"\n",
    "colors[\"cum_errors\"]=\"gray\" \n",
    "colors[\"real_anom\"]=\"gray\"\n",
    "colors[\"det_anomaly\"]=\"indianred\"\n",
    "colors[\"threshold\"]=\"red\"\n",
    "colors[\"c_plus\"]=\"lightsalmon\" \n",
    "colors[\"c_minus\"]=\"springgreen\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67da2a9-d137-45ff-b499-4a969de9bbe3",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801aee2-e5d7-4270-948a-b7163a805934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a case study dataset using pandas\n",
    "method_short=\"ts\" #that is, time serie, or ew, that is elementWise\n",
    "dataset = pd.read_csv(os.path.join(DATA_CLEAN, 'buildings_dataset_clean.csv'))\n",
    "print(\"Shape of dataset: \"+str(dataset.shape))\n",
    "\n",
    "#load dataset description\n",
    "data_description=pd.read_csv(DATASETS+'/buildings_data_description.csv')\n",
    "\n",
    "# load residuals time series\n",
    "file_name=\"residuals\"\n",
    "with open(RESULTS+\"/\"+file_name+\".pkl\", 'rb') as f:\n",
    "    residuals= pickle.load(f)\n",
    "\n",
    "## load ID converter\n",
    "#load dataset description\n",
    "sim_ID_conv= pd.read_excel(os.path.join(DATASETS, \"ID_converter_sim.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287e0d9-44ad-4f11-90cd-e2b23363eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge results and original dataset\n",
    "dataset[\"residuals_semiPar\"]=np.nan\n",
    "dataset[\"residuals_ES\"]=np.nan\n",
    "IDs=dataset.ID.unique()\n",
    "for caseStudy in IDs:\n",
    "    res_ES=residuals[\"ES\", caseStudy]\n",
    "    res_semiPar=residuals[\"semiPar\", caseStudy]\n",
    "    dataset.loc[dataset.ID==caseStudy, \"residuals_ES\"]=res_ES.values\n",
    "    dataset.loc[dataset.ID==caseStudy, \"residuals_semiPar\"]=res_semiPar.values\n",
    "\n",
    "#limitate analysis to simulated caseStudies with anomalies\n",
    "abnormal_buildings_original_ID=sim_ID_conv.loc[[\"anomaly\" in build for build in sim_ID_conv.generator_names], \"my_names\"].values\n",
    "dataset_anomalies=dataset.loc[dataset.ID.isin(abnormal_buildings_original_ID), :]\n",
    "\n",
    "#create a separate dataframe for compartive analysis (anomaly VS no anomaly)\n",
    "normal_buildings_original_ID=sim_ID_conv.loc[[(\"anomaly\" not in build) and (\"retrofit\" not in build) for build in sim_ID_conv.generator_names], \"my_names\"].values\n",
    "dataset_normal=dataset.loc[dataset.ID.isin(normal_buildings_original_ID), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa54f6-554d-4f96-9cc5-57ad34b7d31b",
   "metadata": {},
   "source": [
    "## Check predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d65d36-b06e-4340-bf9e-f772a884f285",
   "metadata": {},
   "source": [
    "#### Full time serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7923fec8-8993-4b9c-84fd-d9491cd1fbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots_IDs=IDs\n",
    "n_plots=len(plots_IDs)\n",
    "\n",
    "if graph_setting==\"article\":\n",
    "    fig_size=big_figsize\n",
    "elif graph_setting==\"notebook\":\n",
    "    fig_size=(long_ts_figsize[0], long_ts_figsize[1]*n_plots)\n",
    "\n",
    "fig, axs = plt.subplots(n_plots, figsize=fig_size)\n",
    "\n",
    "for caseStudy, ax in zip(plots_IDs, axs):\n",
    "    \n",
    "    df=dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    df=df.iloc[:8760, :]\n",
    "    daily_P_real=df.P.rolling(24, min_periods=1).mean()\n",
    "    daily_P_pred=(df.P-df.residuals_semiPar).rolling(24, min_periods=1).mean()\n",
    "    ax.plot(daily_P_real/1000, color=colors[\"P\"])\n",
    "    ax.plot(daily_P_pred/1000, color=colors[\"P_pred\"])\n",
    "    ax.tick_params(labelsize=font_size)\n",
    "    ax.set_ylabel(data_description.loc[data_description[\"Variable_name\"]==\"P\", \"variable_label\"].values[0], fontsize=font_size)\n",
    "    if ax!=axs[1]:\n",
    "        ax.set_xticks([])\n",
    "        \n",
    "    print(caseStudy+\": predictions MAPE: \"+str(mean_absolute_percentage_error(df.P, df.P+df.residuals_semiPar))+\"\\n\")\n",
    "\n",
    "axs[0].text(-0.1, 0.5, 'a)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[0].transAxes)\n",
    "axs[1].text(-0.1, 0.5, 'b)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[1].transAxes)\n",
    "axs[0].legend([\"Real\", \"Predicted\"], fontsize=font_size)\n",
    "axs[1].set_xlabel(\"Time $[h]$\", fontsize=font_size)\n",
    "plt.tight_layout()\n",
    "\n",
    "#fig.savefig(figures_path+\"/pred_ts_results\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d57a8c-904c-48e2-86c9-415cf65e67b2",
   "metadata": {},
   "source": [
    "#### Zoom on winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfd523-650c-4fbc-b51a-36b4dc4d177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_IDs=[IDs[0], IDs[1]]\n",
    "n_plots=len(plots_IDs)\n",
    "\n",
    "if graph_setting==\"article\":\n",
    "    fig_size=big_figsize\n",
    "elif graph_setting==\"notebook\":\n",
    "    fig_size=(long_ts_figsize[0], long_ts_figsize[1]*n_plots)\n",
    "\n",
    "fig, axs = plt.subplots(n_plots, figsize=fig_size)\n",
    "    \n",
    "zoom_on=range(1500,1668)\n",
    "\n",
    "for caseStudy, ax in zip(plots_IDs, axs):\n",
    "    df=dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    df=df.iloc[zoom_on, :]\n",
    "    ax.plot(df.P/1000, color=colors[\"P\"])\n",
    "    ax.plot((df.P-df.residuals_semiPar)/1000, color=colors[\"P_pred\"])\n",
    "    ax.tick_params(labelsize=font_size-2)\n",
    "    ax.set_ylabel(data_description.loc[data_description[\"Variable_name\"]==\"P\", \"variable_label\"].values[0], fontsize=font_size)\n",
    "    if ax!=axs[1]:\n",
    "        ax.set_xticks([])\n",
    "\n",
    "axs[0].text(-0.1, 0.5, 'a)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[0].transAxes)\n",
    "axs[1].text(-0.1, 0.5, 'b)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[1].transAxes)\n",
    "axs[0].legend([\"Real\", \"Predicted\"], fontsize=font_size)\n",
    "axs[1].set_xlabel(\"Time $[h]$\", fontsize=font_size)\n",
    "plt.tight_layout()\n",
    "\n",
    "#fig.savefig(figures_path+\"/pred_ts_results_winter\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67861ed9-81df-4b65-80c5-9025bf56b2e5",
   "metadata": {},
   "source": [
    "#### Zoom on summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06524374-f817-4827-ad71-55a2c1dbd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_IDs=[IDs[0], IDs[1]]\n",
    "n_plots=len(plots_IDs)\n",
    "\n",
    "if graph_setting==\"article\":\n",
    "    fig_size=big_figsize\n",
    "elif graph_setting==\"notebook\":\n",
    "    fig_size=(long_ts_figsize[0], long_ts_figsize[1]*n_plots)\n",
    "\n",
    "fig, axs = plt.subplots(n_plots, figsize=fig_size)\n",
    "\n",
    "zoom_on=range(5000,5168)\n",
    "\n",
    "for caseStudy, ax in zip(plots_IDs, axs):\n",
    "    df=dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    df=df.iloc[zoom_on, :]\n",
    "    ax.plot(df.P/1000, color=colors[\"P\"])\n",
    "    ax.plot((df.P-df.residuals_semiPar)/1000, color=colors[\"P_pred\"])\n",
    "    ax.tick_params(labelsize=font_size-2)\n",
    "    ax.set_ylabel(data_description.loc[data_description[\"Variable_name\"]==\"P\", \"variable_label\"].values[0], fontsize=font_size)\n",
    "    if ax!=axs[1]:\n",
    "        ax.set_xticks([])\n",
    "\n",
    "axs[0].text(-0.1, 0.5, 'a)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[0].transAxes)\n",
    "axs[1].text(-0.1, 0.5, 'b)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[1].transAxes)\n",
    "axs[0].legend([\"Real\", \"Predicted\"], fontsize=font_size)\n",
    "axs[1].set_xlabel(\"Time $[h]$\", fontsize=font_size)\n",
    "plt.tight_layout()\n",
    "\n",
    "#fig.savefig(figures_path+\"/pred_ts_results_summer\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ccfb8-b5fd-4c48-b4c3-93b4732e3b1a",
   "metadata": {},
   "source": [
    "## Check residuals distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92364c77-ed36-4a0d-ae20-89f700d021b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caseStudy=IDs[0]\n",
    "df=dataset.loc[dataset.ID==caseStudy, :]\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(8,6))\n",
    "plot_residuals(df.residuals_ES, df.T_a, ax=ax[0])\n",
    "plot_residuals(df.residuals_semiPar, df.T_a, ax=ax[1])\n",
    "fig.tight_layout()\n",
    "ax[0,0].set_ylabel(\"ES model residuals\")\n",
    "ax[1,0].set_ylabel(\"Semipar model residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48755e2-714f-43e2-9f61-b77e05522357",
   "metadata": {},
   "source": [
    "### Normalize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2b99a-ae9e-4934-ba9a-3a12ed20a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate normalized residuals\n",
    "dataset[\"normalized_residuals_semiPar\"]=np.nan\n",
    "\n",
    "for caseStudy in IDs:\n",
    "    df=dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    df[\"normalized_residuals_semiPar\"]=df[\"residuals_semiPar\"].values/df.P.values\n",
    "    dataset.loc[dataset.ID==caseStudy, \"normalized_residuals_semiPar\"]=df[\"normalized_residuals_semiPar\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151481c4-9347-4bc0-a087-de258deeb22a",
   "metadata": {},
   "source": [
    "#### Compare errors and normalized errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1968bb-d79e-4b72-8543-81ef7211ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(square_figsize[0]*2, square_figsize[1]))\n",
    "\n",
    "caseStudy=IDs[0]\n",
    "\n",
    "for col, ax in zip([\"residuals_semiPar\", \"normalized_residuals_semiPar\"], axs):\n",
    "    df=dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    \n",
    "    if col==\"P\":\n",
    "        df[col]=df[col]/1000\n",
    "        \n",
    "    ax.scatter(df.P, df[col], color=\"gray\", s=2)\n",
    "    ax.tick_params(labelsize=font_size-2)\n",
    "\n",
    "    ax.set_xlabel(data_description.loc[data_description[\"Variable_name\"]==\"T_a\", \"variable_label\"].values[0], fontsize=font_size)\n",
    "\n",
    "    ylims=[-df[col].max(), +df[col].max()]\n",
    "    \n",
    "    if ax==axs[1]:\n",
    "        ylims=[ylims[0]-0.05, ylims[1]+0.05]\n",
    "\n",
    "    ax.set_ylim(ylims)\n",
    "\n",
    "axs[0].set_ylabel(\"Errors $[kW]$\", fontsize=font_size)\n",
    "axs[1].set_ylabel(\"Normalized Errors [-]\", fontsize=font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#fig.savefig(figures_path+\"/scatter_errors_VS_temp\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9657aa-3baf-4e87-818a-493ed0543c2e",
   "metadata": {},
   "source": [
    "## Check cumulative errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f20f4-fc5f-4691-8ed4-0a77759d87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_IDs=IDs\n",
    "n_plots=len(plots_IDs)\n",
    "\n",
    "if graph_setting==\"article\":\n",
    "    fig_size=big_figsize\n",
    "elif graph_setting==\"notebook\":\n",
    "    fig_size=(long_ts_figsize[0], long_ts_figsize[1]*n_plots)\n",
    "\n",
    "fig, axs = plt.subplots(n_plots, figsize=fig_size)\n",
    "\n",
    "for caseStudy, ax in zip(plots_IDs, axs):\n",
    "    df=dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    df=df.iloc[8760:, :]\n",
    "    cum_err=np.cumsum(df.residuals_semiPar/1000)\n",
    "    ax.plot(cum_err, color=colors[\"cum_errors\"])\n",
    "    ax.tick_params(labelsize=font_size-2)\n",
    "    ax.set_ylabel(\"Electrical energy $[kWh]$\", fontsize=font_size)\n",
    "    if ax!=axs[1]:\n",
    "        ax.set_xticks([])\n",
    "    ax.legend([\"Cumulative of errors \\nfor \"+caseStudy], fontsize=font_size-2)\n",
    "    print(\"Cumulated deviation for\"+caseStudy+\" :\"+str(cum_err[-1:].values))\n",
    "    print(\"Total energy demand for\"+caseStudy+\" :\"+str(df.P.sum()/1000))\n",
    "    print(\"Relative deviation for \"+caseStudy+\" :\"+str(cum_err[-1:].values/(df.P.sum())*1000))\n",
    "\n",
    "#axs[0].legend([\"Real\", \"Predicted\"], fontsize=font_size)\n",
    "axs[0].text(-0.1, 0.5, 'a)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[0].transAxes)\n",
    "axs[1].text(-0.1, 0.5, 'b)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[1].transAxes)\n",
    "axs[1].set_xlabel(\"Time $[h]$\", fontsize=font_size)\n",
    "plt.tight_layout()\n",
    "\n",
    "#fig.savefig(figures_path+\"/cum_of_errors_ts_results\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83795499-b531-48ba-b248-299cd34a6117",
   "metadata": {},
   "source": [
    "## Check CUMSUM tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4a21c-5be4-4b10-9388-757dae535745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_IDs=IDs\n",
    "n_plots=len(plots_IDs)\n",
    "\n",
    "if graph_setting==\"article\":\n",
    "    fig_size=big_figsize\n",
    "elif graph_setting==\"notebook\":\n",
    "    fig_size=(long_ts_figsize[0], long_ts_figsize[1]*n_plots)\n",
    "\n",
    "fig, axs = plt.subplots(n_plots, figsize=fig_size)\n",
    "\n",
    "for caseStudy, ax in zip(plots_IDs, axs):\n",
    "    df=dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    res=df[\"residuals_semiPar\"]\n",
    "    norm_res=res/df.P\n",
    "    _, _, res_setup, res_monitor = train_test_split(norm_res, norm_res, test_size=0.33, shuffle=False)\n",
    "    Cpos, Cneg, anomaly_threshold ,sigma= cusum_tab(res_setup, x_monitor=res_monitor, k=1, h=24, moving_range=False)\n",
    "    Cpos=pd.Series(Cpos)\n",
    "    Cneg=pd.Series(Cneg)\n",
    "    \n",
    "    ax.plot(Cpos, label=r\"$C_+$\", color=colors[\"c_plus\"])\n",
    "    ax.plot(Cneg, label=r\"$C_-$\", color=colors[\"c_minus\"])\n",
    "    ax.hlines(y=anomaly_threshold, xmin=Cpos.index[0], xmax=Cpos.index[-1], ls=\"-.\", color=colors[\"threshold\"], label=\"H\")\n",
    "    \n",
    "    # # ax.set_xticks(ax.get_xticks())\n",
    "    # # #ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    # \n",
    "    ax.grid()\n",
    "\n",
    "    ax.tick_params(labelsize=font_size-2)\n",
    "    ax.set_ylabel(\"Electrical energy $[kWh]$\", fontsize=font_size)\n",
    "    if ax!=axs[-1]:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "# axs[0].text(-0.1, 0.5, 'a)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[0].transAxes)\n",
    "# axs[1].text(-0.1, 0.5, 'b)', fontsize=font_size, horizontalalignment='center', verticalalignment='center', transform=axs[1].transAxes)\n",
    "axs[0].legend(fontsize=font_size)\n",
    "axs[-1].set_xlabel(\"Time $[h]$\", fontsize=font_size)\n",
    "plt.tight_layout()\n",
    "\n",
    "#fig.savefig(figures_path+\"/cumsumtab_example\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d471ae-4772-431d-a32d-cbef70e5b71b",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35116c65-510a-4167-be49-6e574e6d8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_IDs=dataset_anomalies.ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909de3ab-c205-42d0-90c3-bb183a4db02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(abnormal_IDs)==0:\n",
    "    print(colored(\"No abnormal events exist in the employed dataset\\nThe remaining cells of the present notebook will not provide any result\", \"red\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d9a9b-14b7-4989-81a9-fe9a6a333f9e",
   "metadata": {},
   "source": [
    "## Tune control chart parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1541aa-57a0-415c-b40e-783190901b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the weight for sensitivity (precision will be 1-w_sens)\n",
    "w_sens=0.8\n",
    "\n",
    "#caseStudy=IDs[0]\n",
    "abnormal_IDs=dataset_anomalies.ID.unique()\n",
    "ks=np.linspace(0.1, 3.5, 18)\n",
    "hs=np.linspace(3, 48, 16)\n",
    "anomaly_detection_score_matrix=pd.DataFrame(index=ks, columns=hs)\n",
    "f1_score_matrix=pd.DataFrame(index=ks, columns=hs)\n",
    "sensitivity_score_matrix=pd.DataFrame(index=ks, columns=hs)\n",
    "precision_score_matrix=pd.DataFrame(index=ks, columns=hs)\n",
    "det_delay_matrix=pd.DataFrame(index=ks, columns=hs)\n",
    "\n",
    "for k in ks:\n",
    "    for h in hs:\n",
    "        df, confusion_matrix_dict, total_con_mat, det_delay = calculate_confusion_matrix(dataset_anomalies, abnormal_IDs, k=k, h=h)\n",
    "        anomaly_detection_score_matrix.loc[k, h] = anomaly_detection_score(total_con_mat, w_sensitivity = w_sens)\n",
    "        f1_score_matrix.loc[k, h] =f1_from_matrix(total_con_mat)\n",
    "        sensitivity_score_matrix.loc[k, h] =sensitivity_from_matrix(total_con_mat)\n",
    "        precision_score_matrix.loc[k, h] =precision_from_matrix(total_con_mat)\n",
    "        det_delay_matrix.loc[k, h] =det_delay.median(axis=1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c96531-7a12-43f5-ada9-72119ba96046",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_detection_score_matrix\n",
    "anomaly_detection_score_matrix.style.applymap(lambda x: \"background-color: palegreen\" if x>0.77 else \"background-color: white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334fd2b-0afa-4afb-b12a-f600380fef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select best configurations\n",
    "n_config=300\n",
    "matrix_copy=det_delay_matrix.copy()\n",
    "best_config_pos={}\n",
    "best_config_params={}\n",
    "\n",
    "for i in range(n_config):\n",
    "    matrix_copy=matrix_copy.fillna(1000)\n",
    "    min_value=matrix_copy.min().min()\n",
    "    idx  = locate_in_df(matrix_copy, min_value)\n",
    "    best_config_params[i]=\"k=\"+str(matrix_copy.index[idx[0]].round(2))+\", h=\"+str(matrix_copy.columns[idx[1]])\n",
    "    matrix_copy.iloc[idx]=1000\n",
    "    # ind = np.unravel_index(np.argmax(matrix_copy, axis=None), matrix_copy.shape)\n",
    "    best_config_pos[i]=idx\n",
    "    # matrix_copy.iloc[ind]=0\n",
    "\n",
    "best_config=pd.DataFrame(index=list(best_config_pos.values()),columns=[\"Anomaly Score\", \"Sensitivity\", \"Precision\", \"Detection delay\"])\n",
    "best_config[\"Parameters\"]=best_config_params.values()\n",
    "\n",
    "for ind in best_config.index:\n",
    "    r , c = ind\n",
    "    best_config.loc[best_config.index==ind, \"Anomaly Score\"]=anomaly_detection_score_matrix.iloc[r, c].round(3)\n",
    "    best_config.loc[best_config.index==ind, \"Sensitivity\"]= sensitivity_score_matrix.iloc[r, c].round(3)\n",
    "    best_config.loc[best_config.index==ind, \"Precision\"]= precision_score_matrix.iloc[r, c].round(3)\n",
    "    best_config.loc[best_config.index==ind, \"Detection delay\"]= det_delay_matrix.iloc[r, c].round(3)\n",
    "\n",
    "best_config=best_config.set_index(\"Parameters\")\n",
    "\n",
    "#select two best configurations from the ones that detect anolmalies rapadly\n",
    "best_config_fast=best_config.loc[best_config.Sensitivity>0.85]\n",
    "best_config_fast=best_config_fast.loc[best_config_fast.Precision>0.1].head(5)\n",
    "\n",
    "## select two best configurations from the ones that have the highest precision and highest sensitivity\n",
    "best_config_precise=best_config.loc[best_config.Sensitivity>0.9]\n",
    "best_config_precise=best_config_precise.loc[best_config_precise.Precision>0.15].sort_values(by=\"Anomaly Score\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0936ec7-81e0-4477-82c9-e9163d507630",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(best_config_fast)\n",
    "display(best_config_precise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88db1ba-c11b-4114-8bc0-f86796e7e529",
   "metadata": {},
   "source": [
    "## Anomaly detection step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a3e7d-2f75-433c-bdb7-60070befeaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_anomalies.loc[:, \"detected_anomaly\"]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b191972-9692-4b0e-a331-3c392e62718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0.9\n",
    "h=42\n",
    "#detect anomaly (it will be done for all case studies)\n",
    "new_dataset, confusion_matrix_dict, total_con_mat, det_delay = calculate_confusion_matrix(dataset_anomalies, abnormal_IDs, k=k, h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac02ed8-8679-4637-93f5-efd03c9e32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471f7a5-c09f-4f4a-9dac-f30b1944a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide anomalies according to type of anomaly\n",
    "n_anomalies=len(set([x.split(\"_\")[1][2] for x in dataset_anomalies.ID.unique()]))\n",
    "n_simulations=int(len(dataset_anomalies.ID.unique())/n_anomalies)\n",
    "det_delay=pd.DataFrame(np.reshape(det_delay, [n_simulations,n_anomalies]), index=range(1, n_simulations+1), columns=[\"Anomaly A\", \"Anomaly B\", \"Anomaly C\", \"Anomaly D\",\"Anomaly E\"])\n",
    "det_delay.index.name=\"Experiment\"\n",
    "\n",
    "det_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b49f3c-406d-4d51-93a9-c00dbf537d19",
   "metadata": {},
   "source": [
    "## Check detection of different types of anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ee99a-2610-43e2-bea2-4633ede82438",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudy_1=\"building_1010\"\n",
    "caseStudy_2=\"building_1020\"\n",
    "caseStudy_3=\"building_1030\"\n",
    "caseStudy_4=\"building_1040\"\n",
    "caseStudy_5=\"building_1050\"\n",
    "\n",
    "dfs_from_generator={}\n",
    "\n",
    "#check magnitude of anomalies (just for anomaly A and B)\n",
    "for caseStudy in [caseStudy_1, caseStudy_2,caseStudy_3, caseStudy_4, caseStudy_5 ]:\n",
    "    dfs_from_generator[caseStudy]=retrive_data_from_generator(caseStudy)\n",
    "\n",
    "print(\"Anomaly A: degradation of COP: \"+str(dfs_from_generator[caseStudy_1].cop_anom_degr.unique().max()*100)+\" %\")\n",
    "print(\"Anomaly B: precentage of rack under maintenance: \"+str((1-dfs_from_generator[caseStudy_2].P_tlc.unique().min()/dfs_from_generator[caseStudy_2].P_tlc.unique().max()).round(4)*100)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086161e-6e98-4354-98c1-50bc3d29a317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3dbf6-a450-4a1b-a5ed-bc7dd52d73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudies=[caseStudy_1,caseStudy_2,caseStudy_3,caseStudy_4,caseStudy_5]\n",
    "\n",
    "for anomaly, caseStudy in zip([\"A\", \"B\", \"C\", \"D\", \"E\"], caseStudies):\n",
    "\n",
    "    if graph_setting==\"article\":\n",
    "        fig_size=big_figsize\n",
    "    elif graph_setting==\"notebook\":\n",
    "        fig_size=(long_ts_figsize[0], long_ts_figsize[1]*4)\n",
    "        \n",
    "    fig, axs = plt.subplots(4, figsize=fig_size)\n",
    "    axs[0].set_title(\"Casestudy: \"+caseStudy)\n",
    "    df=new_dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "    df_normal=dataset_normal.loc[dataset_normal.ID==caseStudy[:-2]+\"00\"].reset_index()\n",
    "    cum_err=np.cumsum(df.residuals_semiPar/1000)\n",
    "    res=df[\"residuals_semiPar\"]\n",
    "    norm_res=res/df.P\n",
    "    _, _, res_setup, res_monitor = train_test_split(norm_res, norm_res, test_size=0.33, shuffle=False)\n",
    "    Cpos, Cneg, anomaly_threshold ,sigma= cusum_tab(res_setup, x_monitor=res_monitor, k=k, h=h, moving_range=False)\n",
    "    Cpos=pd.Series(Cpos)\n",
    "    Cneg=pd.Series(Cneg)\n",
    "\n",
    "    #calculate cumulative deviations (estimated and real)\n",
    "    anom_start_cum_value=cum_err.loc[df.loc[df.anomaly==1, :].index[0]]  #calculate the cumulative deviation starting from the time step of start of the anomaly\n",
    "    cum_est_dev=cum_err-anom_start_cum_value\n",
    "    \n",
    "    real_dev=((df.P-df_normal.P)/1000).cumsum()\n",
    "    anom_start_cum_value=real_dev.loc[df.loc[df.anomaly==1, :].index[0]]  #calculate the cumulative deviation starting from the time step of start of the anomaly\n",
    "    real_dev=real_dev-anom_start_cum_value\n",
    "    \n",
    "    anom_index=df.loc[df.anomaly==1, :].index\n",
    "    zoom_on=range(anom_index.min()-5, anom_index.max()+5)\n",
    "\n",
    "    #calculate percentage of additional consumption\n",
    "    add_power=(df.residuals_semiPar/df.P).loc[range(anom_index.min(), anom_index.max())].mean()\n",
    "    #calculate wasted energy\n",
    "    add_energy=real_dev[anom_index.max()]\n",
    "    est_add_energy=cum_est_dev[anom_index.max()]\n",
    "    \n",
    "    #calculate wasted energy at the time of detection of anomaly\n",
    "    if np.isnan(df.loc[zoom_on].loc[df.detected_anomaly==1, :].index.min())==False:\n",
    "        add_energy_time_of_detection=real_dev[df.loc[zoom_on].loc[df.detected_anomaly==1, :].index.min()]\n",
    "    else:\n",
    "        print(colored(\"WARNING: no anomaly was detected for casestudy \"+caseStudy, \"red\"))\n",
    "        add_energy_time_of_detection=np.zeros(len(real_dev))\n",
    "\n",
    "    #calculate the percentage of wasted energy that would be recovered by immidiate intervenction\n",
    "    \n",
    "    axs[0].plot((df.P/1000).loc[zoom_on], color=colors[\"P\"])\n",
    "    axs[0].plot(((df.P-df.residuals_semiPar)/1000).loc[zoom_on], color=colors[\"P_pred\"])\n",
    "    \n",
    "    axs[1].plot(Cpos.loc[zoom_on], label=r\"$C_+$\", color=colors[\"c_plus\"])\n",
    "    axs[1].plot(Cneg.loc[zoom_on], label=r\"$C_-$\", color=colors[\"c_minus\"])\n",
    "    axs[1].hlines(y=anomaly_threshold, xmin=anom_index.min()-5, xmax=anom_index.max()+5, ls=\"-.\", color=colors[\"threshold\"], label=\"H\")\n",
    "\n",
    "    axs[2].plot(df.anomaly.loc[zoom_on], color=colors[\"real_anom\"])\n",
    "    axs[2].plot(df.detected_anomaly.loc[zoom_on], color=colors[\"det_anomaly\"])\n",
    "\n",
    "    \n",
    "    axs[3].plot(real_dev.loc[zoom_on], color=colors[\"real_anom\"])\n",
    "    axs[3].plot(cum_est_dev.loc[zoom_on], color=colors[\"det_anomaly\"])\n",
    "    \n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.tick_params(labelsize=font_size-2)\n",
    "        ax.grid()\n",
    "        ax.set_xlim(anom_index.min()-5, anom_index.max()+5)\n",
    "        \n",
    "        if ax!=axs[3]:\n",
    "            ax.set_xticklabels([])\n",
    "    \n",
    "    axs[0].set_ylabel(\"Electrical \\nload $[kW]$\", fontsize=font_size)\n",
    "    axs[1].set_ylabel(\"Electrical \\nenergy $[-]$\", fontsize=font_size)\n",
    "    axs[2].set_ylabel(\"Anomaly \\ndetected\", fontsize=font_size)\n",
    "    axs[3].set_ylabel(\"Deviation of\\n electrical \\nenergy $[kWh]$\", fontsize=font_size)\n",
    "\n",
    "\n",
    "    for ax in [axs[0], axs[2], axs[3]]:\n",
    "        ax.legend([\"Real\", \"Predicted\"], fontsize=font_size-2)\n",
    "\n",
    "    axs[1].legend(fontsize=font_size)\n",
    "    axs[3].set_xlabel(\"Time $[h]$\", fontsize=font_size-2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #fig.savefig(figures_path+\"/detection_of_anomaly_\"+anomaly, bbox_inches='tight', dpi=200)\n",
    "\n",
    "    print(\"Mean additional power: \"+str(add_power))\n",
    "    print(\"Wasted energy: Predicted / Real : \"+str(est_add_energy)+\" / \"+str(add_energy))\n",
    "    print(\"Wasted energy at the time of anomaly detection: \"+ str(add_energy_time_of_detection))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebbf4a-f6e1-4a56-a0f3-0ca4d3f265dc",
   "metadata": {},
   "source": [
    "## Check the undetected anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb7826-4b6d-4736-84bb-4e346a5431df",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseStudy=\"building_1010\"\n",
    "\n",
    "#check magnitude of anomalies (just for anomaly A and B)\n",
    "dfs_from_generator[caseStudy]=retrive_data_from_generator(caseStudy)\n",
    "\n",
    "print(\"Anomaly A: degradation of COP: \"+str(dfs_from_generator[caseStudy].cop_anom_degr.unique().max()*100)+\" %\")\n",
    "    \n",
    "if graph_setting==\"article\":\n",
    "    fig_size=big_figsize\n",
    "elif graph_setting==\"notebook\":\n",
    "    fig_size=(long_ts_figsize[0], long_ts_figsize[1]*4)\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=fig_size)\n",
    "\n",
    "axs[0].set_title(\"Casestudy: \"+caseStudy)\n",
    "\n",
    "df=new_dataset.loc[dataset.ID==caseStudy].reset_index()\n",
    "df_normal=dataset_normal.loc[dataset_normal.ID==caseStudy[:-2]+\"00\"].reset_index()\n",
    "cum_err=np.cumsum(df.residuals_semiPar/1000)\n",
    "res=df[\"residuals_semiPar\"]\n",
    "norm_res=res/df.P\n",
    "_, _, res_setup, res_monitor = train_test_split(norm_res, norm_res, test_size=0.33, shuffle=False)\n",
    "Cpos, Cneg, anomaly_threshold ,sigma= cusum_tab(res_setup, x_monitor=res_monitor, k=k, h=h, moving_range=False)\n",
    "Cpos=pd.Series(Cpos)\n",
    "Cneg=pd.Series(Cneg)\n",
    "\n",
    "#calculate cumulative deviations (estimated and real)\n",
    "anom_start_cum_value=cum_err.loc[df.loc[df.anomaly==1, :].index[0]]  #calculate the cumulative deviation starting from the time step of start of the anomaly\n",
    "cum_est_dev=cum_err-anom_start_cum_value\n",
    "\n",
    "real_dev=((df.P-df_normal.P)/1000).cumsum()\n",
    "anom_start_cum_value=real_dev.loc[df.loc[df.anomaly==1, :].index[0]]  #calculate the cumulative deviation starting from the time step of start of the anomaly\n",
    "real_dev=real_dev-anom_start_cum_value\n",
    "\n",
    "anom_index=df.loc[df.anomaly==1, :].index\n",
    "zoom_on=range(anom_index.min()-5, anom_index.max()+5)\n",
    "\n",
    "#calculate percentage of additional consumption\n",
    "add_power=(df.residuals_semiPar/df.P).loc[range(anom_index.min(), anom_index.max())].mean()\n",
    "#calculate wasted energy\n",
    "add_energy=real_dev[anom_index.max()]\n",
    "est_add_energy=cum_est_dev[anom_index.max()]\n",
    "\n",
    "axs[0].plot((df.P/1000).loc[zoom_on], color=colors[\"P\"])\n",
    "axs[0].plot(((df.P-df.residuals_semiPar)/1000).loc[zoom_on], color=colors[\"P_pred\"])\n",
    "\n",
    "axs[1].plot(Cpos.loc[zoom_on], label=r\"$C_+$\", color=colors[\"c_plus\"])\n",
    "axs[1].plot(Cneg.loc[zoom_on], label=r\"$C_-$\", color=colors[\"c_minus\"])\n",
    "axs[1].hlines(y=anomaly_threshold, xmin=anom_index.min(), xmax=anom_index.max(), ls=\"-.\", color=colors[\"threshold\"], label=\"H\")\n",
    "\n",
    "axs[2].plot(df.anomaly.loc[zoom_on], color=colors[\"real_anom\"])\n",
    "axs[2].plot(df.detected_anomaly.loc[zoom_on], color=colors[\"det_anomaly\"])\n",
    "\n",
    "\n",
    "axs[3].plot(real_dev.loc[zoom_on], color=colors[\"real_anom\"])\n",
    "axs[3].plot(cum_est_dev.loc[zoom_on], color=colors[\"det_anomaly\"])\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(labelsize=font_size-2)\n",
    "    ax.grid()\n",
    "    ax.set_xlim(anom_index.min()-5, anom_index.max()+5)\n",
    "    \n",
    "    if ax!=axs[3]:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "axs[0].set_ylabel(\"Electrical \\nload $[kW]$\", fontsize=font_size)\n",
    "axs[1].set_ylabel(\"Electrical \\nenergy $[-]$\", fontsize=font_size)\n",
    "axs[2].set_ylabel(\"Anomaly \\ndetected\", fontsize=font_size)\n",
    "axs[3].set_ylabel(\"Deviation of\\n electrical \\nenergy $[kWh]$\", fontsize=font_size)\n",
    "\n",
    "\n",
    "for ax in [axs[0], axs[2], axs[3]]:\n",
    "    ax.legend([\"Real\", \"Predicted\"], fontsize=font_size-2)\n",
    "\n",
    "axs[1].legend(fontsize=font_size)\n",
    "axs[3].set_xlabel(\"Time $[h]$\", fontsize=font_size-2)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(figures_path+\"/undetected_anomaly\", bbox_inches='tight', dpi=200)\n",
    "\n",
    "print(\"Mean additional power: \"+str(add_power))\n",
    "print(\"Wasted energy: Predicted / Real : \"+str(est_add_energy)+\" / \"+str(add_energy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f5520-8fe5-47a3-979b-469055f29b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
