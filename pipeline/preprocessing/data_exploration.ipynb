{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "#import wget\n",
    "#import constants as const \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import median_abs_deviation\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "PROJECT_DIR =Path(os.path.abspath('')).parents[1]\n",
    "\n",
    "sys.path.append(os.fspath(PROJECT_DIR))\n",
    "from pipeline.definitions import *\n",
    "from pipeline.preprocessing.data_preprocessing import statistical_prepro\n",
    "from pipeline.preprocessing_fx import data_exploration, check_outliers\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "import matplotlib.colors as mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select graphic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_setting=\"notebook\" #or \"article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph_setting==\"article\":\n",
    "    \n",
    "    #journal-quality parameter settings\n",
    "    resolution_factor=2\n",
    "    desired_font=10\n",
    "\n",
    "elif graph_setting==\"notebook\":\n",
    "    resolution_factor=1\n",
    "    desired_font=12\n",
    "    \n",
    "#conversion factors\n",
    "cm_to_inch=0.393701\n",
    "classic_proportion=6.4/4.8\n",
    "golden_rate=1.618\n",
    "\n",
    "#Elsevier column width is 8.4 cm, double-column width is 17.7 cm (in inches: 3.31 and 6.97)\n",
    "small_figsize=(resolution_factor*3.31, resolution_factor*3.31/classic_proportion)\n",
    "big_figsize=(resolution_factor*6.97, resolution_factor*6.97/classic_proportion)\n",
    "\n",
    "#changings regarding fonttypex\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['font.family'] = \"Arial\"\n",
    "\n",
    "font_size=resolution_factor*desired_font\n",
    "\n",
    "\n",
    "#define path for figures\n",
    "figures_path=FIGURES\n",
    "#check existance of figure path\n",
    "if not os.path.exists(figures_path):\n",
    "    print(\"The selected directory to store figures does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset and load it as a Pandas DataFrame\n",
    "raw_data = pd.read_csv(os.path.join(DATA_RAW,'all_buildings_dataset'))\n",
    "print(\"Data shape:\")\n",
    "print(raw_data.shape)\n",
    "n_keys=len(raw_data.ID.unique())\n",
    "print(\"\\nThe dataset contains \"+str(n_keys)+\" case studies\\n\")\n",
    "print(\"Case studies contain an average of \"+str(int(raw_data.shape[0]/n_keys))+\" time steps and \"+str(raw_data.shape[1]-1)+\" variables.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns #check dataset variables names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset is provided with hourly data resolution, hence it may be easily seen that it corresponds to two years of measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select columns \n",
    "data = raw_data.copy()\n",
    "print('Complete list of column names:')\n",
    "print(data.columns.values)\n",
    "print(\"Data description: \")\n",
    "data_description= pd.read_csv(os.path.join(DATASETS,'buildings_data_description.csv'))\n",
    "display(data_description.drop(\"Comment\", axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check statistics of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting NaN values in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = data.isna().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and I/O variables definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns with many NaN values and then remove elements with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['UV'],axis=1)\n",
    "\n",
    "# Drop out all instances with NaN values\n",
    "data = data.dropna(axis=0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the datasets shape for each single case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdatasets_info=data.groupby(\"ID\").describe()\n",
    "subdatasets_info[('T_a', 'count')].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input(s) and output(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var=['P']\n",
    "# include all the other variables in the x datasets (inputs), except the case study key (ID)\n",
    "ind_var= [var for var in list(data.columns) if ((var not in dep_var) and (var not in [\"ID\"] ))]\n",
    "#create dataframes\n",
    "X_data = data[ind_var]\n",
    "Y_data = data[dep_var]\n",
    "name_data = data['ID']\n",
    "\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "X_data = X_data.to_numpy(dtype='float64')\n",
    "Y_data = Y_data.to_numpy(dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the disribution of some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=['T_a', 'P', 'T_b', 'Wv', 'G', 's_H', 'c_H']\n",
    "#fig, axs=plt.subplots(int(len(variables)/2), 2, figsize=(6.4, 1.2*len(variables)))\n",
    "fig, axs=plt.subplots(int(len(variables)/3), 3, figsize=big_figsize)\n",
    "for var, ax in zip(variables, axs.flatten()):\n",
    "    data_exploration(data, data_description, var, subplot=ax, caseStudy=\"random\", font_size=font_size)\n",
    "    \n",
    "plt.suptitle(\"Data distribution from a single case study\", fontsize=font_size)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select variables for correlation analysis\n",
    "#columns_for_corr=['T_a', 'P', 'T_b', 'G', 'dayType', 'T_eq_3', 'T_eq_6', 'T_eq_12', 'T_eq_24']\n",
    "columns_for_corr=['T_a', 'P', 'T_b', 'DP', 'RH', 'Wv', 'Wgv', 'atmP', 'G', 's_Wa', 'c_Wa',\n",
    "       's_H', 'c_H', 'dayType', 's_D', 'c_D', 'T_eq_3',\n",
    "       'T_eq_6', 'T_eq_12', 'T_eq_24']\n",
    "#dsiplay one random example\n",
    "IDs=data.ID.unique()\n",
    "fig, axs=plt.subplots(1, figsize=(4.8, 4))\n",
    "caseStudy=random.choice(IDs)\n",
    "df_corr=data.loc[data.ID==caseStudy,  columns_for_corr].corr()\n",
    "#display(df_corr.round(2))\n",
    "sns.heatmap(df_corr, vmin=0.0, vmax=1.0, ax=axs)\n",
    "plt.title(\"Sample correlation Matrix\")\n",
    "#display all the subcases\n",
    "fig, axs=plt.subplots(int(len(IDs)/6), 6, figsize=(9, 0.25*len(IDs)))\n",
    "for caseStudy, ax in zip(IDs, axs.flatten()):\n",
    "    df_corr=data.loc[data.ID==caseStudy, columns_for_corr].corr()\n",
    "    #display(df_corr.round(2))\n",
    "    sns.heatmap(df_corr, vmin=0.0, vmax=1.0, ax=ax, cbar=False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.suptitle(\"Correlation matrix from all the case studies\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples=pd.DataFrame([ [ \"T_a\",\"P\"], [ \"G\", \"P\"], [ \"Wv\", \"P\"], [ \"T_a\", \"T_b\"],[ \"Wgv\", \"Wv\"],[ \"DP\", \"T_a\"]])\n",
    "\n",
    "var1_list=list(tuples.iloc[:,0].values)                    \n",
    "var2_list=list(tuples.iloc[:,1].values)\n",
    "fig, axs=plt.subplots(int(np.shape(tuples)[0]/3), 3, figsize=big_figsize)\n",
    "\n",
    "for var1, var2, ax in zip(var1_list, var2_list, axs.flatten()):\n",
    "    data_exploration(data, data_description, var1, var2=var2, subplot=ax, font_size=font_size-2)\n",
    "plt.suptitle(\"Data distribution from a single case study\", fontsize=font_size)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "88688c6e448969839ee4163a6b670935eee153233d49c269b97f67337e0696f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
